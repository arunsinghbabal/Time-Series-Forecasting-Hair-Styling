{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa11f683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import pmdarima as pm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pmdarima.arima.utils import ndiffs,nsdiffs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a826cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset from the excel file\n",
    "file=pd.read_excel('sample.xlsx',sheet_name='Data',index_col=0)\n",
    "def data_cleaning(data):\n",
    "    \"\"\"\n",
    "        Method Name: data_cleaning\n",
    "        Description: This function carried out data_cleaing by removing special characters \n",
    "        from the dataset and arranging the data in an orderly manner.\n",
    "        Output: df_store= DataFrame containing initial data\n",
    "    \"\"\"\n",
    "    data=data.T #Transpose the data\n",
    "    to_remove=['[',']',','] # Characters to remove from the strings of the feature\n",
    "    for v in to_remove:\n",
    "        for l in range (data.shape[0]):\n",
    "            for m in range(data.shape[1]):\n",
    "                value=data.iloc[l,m].replace(v,'')\n",
    "                data.iloc[l,m]=value\n",
    "    df_store=[]\n",
    "    for i in range(len(data.columns)):\n",
    "        # Split the data in every feature by a single space\n",
    "        a=data.iloc[0,i].split(' ')\n",
    "        b=data.iloc[1,i].split(' ')\n",
    "        df=pd.DataFrame([a,b]).T\n",
    "        df.columns=['ds','y'] # Assign the column name of the dataframe\n",
    "        df=df.astype('float64') # Change the dataset type to float\n",
    "        df['ds']=pd.to_datetime(df['ds'], unit='s') # Make the feature datatype as datetime\n",
    "        df.to_csv('sample{x}_time.csv'.format(x=i)) # Save the dataset into csv format\n",
    "        df_store.append(df)\n",
    "    return df_store\n",
    "\n",
    "\n",
    "class Visualize:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def adfuller_test(self, value):\n",
    "        \"\"\"\n",
    "        Method Name: adfuller_test\n",
    "        Description: This function carries out a statistical adfuller test and identifies if the data is\n",
    "        stationary or non-stationary.\n",
    "        Output: None\n",
    "        \"\"\"\n",
    "        result=adfuller(value)\n",
    "        labels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations Used']\n",
    "        for value,label in zip(result,labels):\n",
    "            print(label+' : '+str(value) )\n",
    "        if result[1] <= 0.05:\n",
    "            print(\"strong evidence against the null hypothesis(Ho), reject the null hypothesis. Data has no unit root and is stationary\")\n",
    "        else:\n",
    "            print(\"weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \")\n",
    "    \n",
    "    def data_visualize(self,data):\n",
    "        \"\"\"\n",
    "        Method Name: data_visualize\n",
    "        Description: This function crop the unnecessary intial part of the dataset.\n",
    "        Output: df=clean dataset\n",
    "                true_val_df='y' column values for sample where 'ds' column is null\n",
    "        \"\"\"\n",
    "        true_val_df=[]\n",
    "        df=[]\n",
    "        for i,d in enumerate(data):\n",
    "            true_val_df.append(d.loc[d['ds'].isnull()]['y'])\n",
    "            # remove outlier data from the samples\n",
    "            if i==0:\n",
    "                d=d.loc[~(d['ds']=='1970-01-01 04:29:36')]\n",
    "            elif i==1:\n",
    "                d=d.loc[~(d['ds']=='1970-01-01 04:29:28.000')]\n",
    "            d.dropna(inplace=True)\n",
    "            df.append(d)\n",
    "            #d=d.set_index('ds')\n",
    "            d.reset_index(inplace=True)\n",
    "            d.drop(['ds','index'],axis=1,inplace=True)\n",
    "            # visualize the samples\n",
    "            d.plot()\n",
    "            plt.title('Sample {x}'.format(x=i+1))\n",
    "            plt.xlabel('Time')\n",
    "            plt.ylabel('Power in watt')\n",
    "            #plt.savefig('sample_{x}_{y}.png'.format(x=i,y=message))\n",
    "            plt.show()\n",
    "            print('\\nadfuller results for sample 1\\n')\n",
    "            self.adfuller_test(df[i]['y']) # check if data is stationary or not\n",
    "        return df,true_val_df\n",
    "    \n",
    "\n",
    "def MAE(y,yhat):\n",
    "    \"\"\"\n",
    "        Method Name: MAE\n",
    "        Description: This function calculates the mean-squared error for the model by taking into consideration of \n",
    "        actual and the predicted values. \n",
    "        Output: df_store= mae=Mean_squared_error value\n",
    "    \"\"\"\n",
    "    diff = np.abs(np.array(y)-np.array(yhat))\n",
    "    try:\n",
    "        mae =  round(np.mean(np.fabs(diff)),3)\n",
    "    except:\n",
    "        print(\"Error while calculating\")\n",
    "        mae = np.nan\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df384bcd",
   "metadata": {},
   "source": [
    "# Clean the sample data, visuallize it and check if stationary or non-stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c07671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual=Visualize()\n",
    "df_stg=data_cleaning(file)\n",
    "df=[]\n",
    "df.append(df_stg[0][190:])\n",
    "df.append(df_stg[1][190:])\n",
    "df.append(df_stg[2][190:])\n",
    "df,true_val_df_vs4=visual.data_visualize(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079390bd",
   "metadata": {},
   "source": [
    "# Identify the p and q values for the samples for arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77480e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,v in enumerate(df):\n",
    "    print('sample {}'.format(i+1))\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    ax1 = fig.add_subplot(211)\n",
    "    fig = sm.graphics.tsa.plot_acf(v['y'].copy(),ax=ax1,lags=250)  # plot the autocorrelation\n",
    "    ax2 = fig.add_subplot(212)\n",
    "    fig = sm.graphics.tsa.plot_pacf(v['y'].dropna(),ax=ax2) # plot the partial autocorrelation\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb43d11",
   "metadata": {},
   "source": [
    "# Arima Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abcfe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "arima_score={'sample':[],'mean_score':[],'MAE_score':[],'p':[],'d':[],'q':[]}\n",
    "sample_1_p=[1,17]\n",
    "sample_2_p=[1,7,11]\n",
    "sample_3_p=[2,8]\n",
    "sample_1_q=[21]\n",
    "sample_2_q=[21,18]\n",
    "sample_3_q=[40,34,37]\n",
    "arima_pram={'0':[sample_1_p,sample_1_q,0],'1':[sample_2_p,sample_2_q,0],'2':[sample_3_p,sample_3_q,0]}\n",
    "for i,j in arima_pram.items():\n",
    "    arima_train=pd.DataFrame(df[int(i)]['y'][:-20].copy())\n",
    "    arima_test=pd.DataFrame(df[int(i)]['y'][-21:].copy())\n",
    "    for m in j[0]:\n",
    "        for n in j[1]:\n",
    "            print('Sample{}'.format(int(i)+1)\n",
    "            model=sm.tsa.ARIMA(arima_train['y'],order=(m,j[1],n))\n",
    "            model=model_ARIMA.fit()\n",
    "            print(model.summary())\n",
    "            pred=model.predict(start=arima_test.index[0],end=arima_test.index[-1])\n",
    "            mae_score=MAE(arima_test,pred)\n",
    "            mean_score=arima_train['y'].mean()\n",
    "            sample_name='Sample_{}'.format(int(i)+1)\n",
    "            model_score['sample'].append(sample_name)\n",
    "            model_score['mean_score'].append(mean_score)\n",
    "            model_score['MAE_score'].append(mae_score)\n",
    "            model_score['p'].append(m)\n",
    "            model_score['d'].append(j[1])\n",
    "            model_score['q'].append(n)\n",
    "            plt.plot(arima_test.index,pred,label='Prediction')\n",
    "            plt.plot(arima_test.index,arima_train['y'][-21:],label='Original')\n",
    "            plt.title('Sample_1 mean value={y}, MAE value={z},p_value={a},d_value={b},q_value={c}'.format(y=mean_score,z=mae_score,a=m,b=0,c=n))\n",
    "            plt.legend(loc=\"upper left\")\n",
    "            plt.xlabel('Time')\n",
    "            plt.ylabel('Power / watt')\n",
    "            #plt.savefig('sample_1_predicted_arima_p={a}_d={b}_q={c}.png'.format(a=i,b=d,c=j))\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bcc972",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba32999c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_fun(n=50,m=100):\n",
    "    model=Sequential()\n",
    "    model.add(LSTM(n,return_sequences=True,input_shape=(m,1)))\n",
    "    model.add(LSTM(n,return_sequences=True))\n",
    "    model.add(LSTM(n))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error',optimizer='adam',metrics=['accuracy'])\n",
    "    return model\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-time_step-1):\n",
    "        a = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100 \n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + time_step, 0])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44defd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_l=[]\n",
    "df_l.append(df_stg[0][190:])\n",
    "df_l.append(df_stg[1][190:])\n",
    "df_l.append(df_stg[2][190:])\n",
    "true_df_array_stk=[]\n",
    "array_stk=[]\n",
    "for i,a in enumerate(df_vs4):\n",
    "    true_df_array_stk.append(a.loc[a['ds'].isnull()])\n",
    "    # Remove the outliers in the dataset\n",
    "    if i==0:\n",
    "        a=a.loc[~(a['ds']=='1970-01-01 04:29:36')]\n",
    "    elif i==1:\n",
    "        a=a.loc[~(a['ds']=='1970-01-01 04:29:28.000')]\n",
    "    a.dropna(inplace=True)\n",
    "    a=np.array(a['y']).reshape(-1,1) # create an array and reshape it\n",
    "    array_stk.append(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c12b183",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_score={'train_error':[],'test_error':[],'time_step':[]}\n",
    "lstm_model_stk=[]\n",
    "for i,arr_data in enumerate(array_stk):\n",
    "    # perform minmax scaler on dataset\n",
    "    scaler=MinMaxScaler(feature_range=(0,1))\n",
    "    scaled_arr=scaler.fit_transform(arr_data)\n",
    "    \n",
    "    training_size=int(len(scaled_arr)*0.7)\n",
    "    test_size=len(scaled_arr)-training_size\n",
    "    time_step=100\n",
    "    # Extract test and train data\n",
    "    train_data=scaled_arr[0:training_size,:]\n",
    "    test_data=scaled_arr[training_size:len(scaled_arr),:1]\n",
    "    X_train, y_train = create_dataset(train_data, time_step)\n",
    "    X_test, ytest = create_dataset(test_data, time_step)\n",
    "    X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)\n",
    "    X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)\n",
    "    # Train and predict from the model\n",
    "    model=lstm_fun(n=50,m=time_step)\n",
    "    model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=100,batch_size=64,verbose=1)\n",
    "    train_predict=model.predict(X_train)\n",
    "    test_predict=model.predict(X_test)\n",
    "    # Calculate the mean squared error and mean of the test data\n",
    "    mae_score=MAE(ytest,test_predict)\n",
    "    mean_s=np.mean(ytest)\n",
    "    print(mae_score)\n",
    "    print('mean',mean_s)\n",
    "    # inverse transform the dependent feature\n",
    "    train_predict=scaler.inverse_transform(train_predict)\n",
    "    test_predict=scaler.inverse_transform(test_predict)\n",
    "    train_error=math.sqrt(mean_squared_error(y_train,train_predict))\n",
    "    test_error=math.sqrt(mean_squared_error(ytest,test_predict))\n",
    "    # add score in a dictionary\n",
    "    neural_score['train_error'].append(train_error)\n",
    "    neural_score['test_error'].append(test_error)\n",
    "    neural_score['time_step'].append(time_step)\n",
    "    look_back=100\n",
    "    trainPredictPlot = np.empty_like(scaled_arr)\n",
    "    trainPredictPlot[:, :] = np.nan\n",
    "    trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict\n",
    "    # shift test predictions for plotting\n",
    "    testPredictPlot = np.empty_like(scaled_arr)\n",
    "    testPredictPlot[:, :] = np.nan\n",
    "    print(len(test_predict))\n",
    "    testPredictPlot[len(train_predict)+(look_back*2)+1:(len(scaled_arr)-1), :] = test_predict\n",
    "    # plot baseline and predictions\n",
    "    plt.plot(scaler.inverse_transform(scaled_arr),label='Original', color='b')\n",
    "    plt.plot(trainPredictPlot,label='train_predicted', color='g')\n",
    "    plt.plot(testPredictPlot,label='test_predicted', color='r')\n",
    "    plt.title('time_step={x}, train_error={y} and test_error= {z}'.format(x=time_step,y=train_error,z=test_error))\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.xlabel('Time / 2*ms')\n",
    "    plt.ylabel('Power / watt')\n",
    "    plt.savefig('sample_{x}_predicted_lstm_{y}_time_step.png'.format(x=i,y=time_step))\n",
    "    plt.show()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deebb841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4594df2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30bf900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03718b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
