{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aa6f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import pmdarima as pm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pmdarima.arima.utils import ndiffs,nsdiffs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979493ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset from the excel file\n",
    "file=pd.read_excel('sample.xlsx',sheet_name='Data',index_col=0)\n",
    "def data_cleaning(data):\n",
    "    \"\"\"\n",
    "        Method Name: data_cleaning\n",
    "        Description: This function carried out data_cleaing by removing special characters \n",
    "        from the dataset and arranging the data in an orderly manner.\n",
    "        Output: df_store= DataFrame containing initial data\n",
    "    \"\"\"\n",
    "    data=data.T #Transpose the data\n",
    "    to_remove=['[',']',','] # Characters to remove from the strings of the feature\n",
    "    for v in to_remove:\n",
    "        for l in range (data.shape[0]):\n",
    "            for m in range(data.shape[1]):\n",
    "                value=data.iloc[l,m].replace(v,'')\n",
    "                data.iloc[l,m]=value\n",
    "    df_store=[]\n",
    "    for i in range(len(data.columns)):\n",
    "        # Split the data in every feature by a single space\n",
    "        a=data.iloc[0,i].split(' ')\n",
    "        b=data.iloc[1,i].split(' ')\n",
    "        df=pd.DataFrame([a,b]).T\n",
    "        df.columns=['ds','y'] # Assign the column name of the dataframe\n",
    "        df=df.astype('float64') # Change the dataset type to float\n",
    "        df['ds']=pd.to_datetime(df['ds'], unit='s') # Make the feature datatype as datetime\n",
    "        df.to_csv('sample{x}_time.csv'.format(x=i)) # Save the dataset into csv format\n",
    "        df_store.append(df)\n",
    "    return df_store\n",
    "\n",
    "\n",
    "class Visualize:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def adfuller_test(self, value):\n",
    "        \"\"\"\n",
    "        Method Name: adfuller_test\n",
    "        Description: This function carries out a statistical adfuller test and identifies if the data is\n",
    "        stationary or non-stationary.\n",
    "        Output: None\n",
    "        \"\"\"\n",
    "        result=adfuller(value)\n",
    "        labels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations Used']\n",
    "        for value,label in zip(result,labels):\n",
    "            print(label+' : '+str(value) )\n",
    "        if result[1] <= 0.05:\n",
    "            print(\"strong evidence against the null hypothesis(Ho), reject the null hypothesis. Data has no unit root and is stationary\")\n",
    "        else:\n",
    "            print(\"weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \")\n",
    "    \n",
    "    def data_visualize(self,data):\n",
    "        \"\"\"\n",
    "        Method Name: data_visualize\n",
    "        Description: This function crop the unnecessary intial part of the dataset.\n",
    "        Output: df=clean dataset\n",
    "                true_val_df='y' column values for sample where 'ds' column is null\n",
    "        \"\"\"\n",
    "        true_val_df=[]\n",
    "        df=[]\n",
    "        for i,d in enumerate(data):\n",
    "            true_val_df.append(d.loc[d['ds'].isnull()]['y'])\n",
    "            # remove outlier data from the samples\n",
    "            if i==0:\n",
    "                d=d.loc[~(d['ds']=='1970-01-01 04:29:36')]\n",
    "            elif i==1:\n",
    "                d=d.loc[~(d['ds']=='1970-01-01 04:29:28.000')]\n",
    "            d.dropna(inplace=True)\n",
    "            df.append(d)\n",
    "            #d=d.set_index('ds')\n",
    "            d.reset_index(inplace=True)\n",
    "            d.drop(['ds','index'],axis=1,inplace=True)\n",
    "            # visualize the samples\n",
    "            d.plot()\n",
    "            plt.title('Sample {x}'.format(x=i+1))\n",
    "            plt.xlabel('Time')\n",
    "            plt.ylabel('Power in watt')\n",
    "            #plt.savefig('sample_{x}_{y}.png'.format(x=i,y=message))\n",
    "            plt.show()\n",
    "            print('\\nadfuller results for sample 1\\n')\n",
    "            self.adfuller_test(df[i]['y']) # check if data is stationary or not\n",
    "        return df,true_val_df\n",
    "    \n",
    "\n",
    "def MAE(y,yhat):\n",
    "    \"\"\"\n",
    "        Method Name: MAE\n",
    "        Description: This function calculates the mean-squared error for the model by taking into consideration of \n",
    "        actual and the predicted values. \n",
    "        Output: df_store= mae=Mean_squared_error value\n",
    "    \"\"\"\n",
    "    diff = np.abs(np.array(y)-np.array(yhat))\n",
    "    try:\n",
    "        mae =  round(np.mean(np.fabs(diff)),3)\n",
    "    except:\n",
    "        print(\"Error while calculating\")\n",
    "        mae = np.nan\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b676a8",
   "metadata": {},
   "source": [
    "# Clean the sample data, visuallize it and check if stationary or non-stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31433eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual=Visualize()\n",
    "df_stg=data_cleaning(file)\n",
    "df=[]\n",
    "df.append(df_stg[0][190:])\n",
    "df.append(df_stg[1][190:])\n",
    "df.append(df_stg[2][190:])\n",
    "df,true_val_df_vs4=visual.data_visualize(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9125ccee",
   "metadata": {},
   "source": [
    "# Identify the p and q values for the samples for arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da7a36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,v in enumerate(df):\n",
    "    print('sample {}'.format(i+1))\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    ax1 = fig.add_subplot(211)\n",
    "    fig = sm.graphics.tsa.plot_acf(v['y'].copy(),ax=ax1,lags=250)  # plot the autocorrelation\n",
    "    ax2 = fig.add_subplot(212)\n",
    "    fig = sm.graphics.tsa.plot_pacf(v['y'].dropna(),ax=ax2) # plot the partial autocorrelation\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3c86ef",
   "metadata": {},
   "source": [
    "# Arima Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63db6b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "arima_score={'sample':[],'mean_score':[],'MAE_score':[],'p':[],'d':[],'q':[]}\n",
    "sample_1_p=[1,17]\n",
    "sample_2_p=[1,7,11]\n",
    "sample_3_p=[2,8]\n",
    "sample_1_q=[21]\n",
    "sample_2_q=[21,18]\n",
    "sample_3_q=[40,34,37]\n",
    "arima_pram={'0':[sample_1_p,sample_1_q,0],'1':[sample_2_p,sample_2_q,0],'2':[sample_3_p,sample_3_q,0]}\n",
    "for i,j in arima_pram.items():\n",
    "    arima_train=pd.DataFrame(df[int(i)]['y'][:-20].copy())\n",
    "    arima_test=pd.DataFrame(df[int(i)]['y'][-21:].copy())\n",
    "    for m in j[0]:\n",
    "        for n in j[1]:\n",
    "            print('Sample{}'.format(int(i)+1)\n",
    "            model=sm.tsa.ARIMA(arima_train['y'],order=(m,j[1],n))\n",
    "            model=model_ARIMA.fit()\n",
    "            print(model.summary())\n",
    "            pred=model.predict(start=arima_test.index[0],end=arima_test.index[-1])\n",
    "            mae_score=MAE(arima_test,pred)\n",
    "            mean_score=arima_train['y'].mean()\n",
    "            sample_name='Sample_{}'.format(int(i)+1)\n",
    "            model_score['sample'].append(sample_name)\n",
    "            model_score['mean_score'].append(mean_score)\n",
    "            model_score['MAE_score'].append(mae_score)\n",
    "            model_score['p'].append(m)\n",
    "            model_score['d'].append(j[1])\n",
    "            model_score['q'].append(n)\n",
    "            plt.plot(arima_test.index,pred,label='Prediction')\n",
    "            plt.plot(arima_test.index,arima_train['y'][-21:],label='Original')\n",
    "            plt.title('Sample_1 mean value={y}, MAE value={z},p_value={a},d_value={b},q_value={c}'.format(y=mean_score,z=mae_score,a=m,b=0,c=n))\n",
    "            plt.legend(loc=\"upper left\")\n",
    "            plt.xlabel('Time')\n",
    "            plt.ylabel('Power / watt')\n",
    "            #plt.savefig('sample_1_predicted_arima_p={a}_d={b}_q={c}.png'.format(a=i,b=d,c=j))\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb337fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6642296d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c5b4d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
